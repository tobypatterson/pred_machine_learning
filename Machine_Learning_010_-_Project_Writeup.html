<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Toby Patterson" />

<meta name="date" content="2015-01-23" />

<title>Machine Learning 010 Project Writeup</title>

<script src="Machine_Learning_010_-_Project_Writeup_files/jquery-1.11.0/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link href="Machine_Learning_010_-_Project_Writeup_files/bootstrap-2.3.2/css/bootstrap.min.css" rel="stylesheet" />
<link href="Machine_Learning_010_-_Project_Writeup_files/bootstrap-2.3.2/css/bootstrap-responsive.min.css" rel="stylesheet" />
<script src="Machine_Learning_010_-_Project_Writeup_files/bootstrap-2.3.2/js/bootstrap.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="Machine_Learning_010_-_Project_Writeup_files/highlight/default.css"
      type="text/css" />
<script src="Machine_Learning_010_-_Project_Writeup_files/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
</style>
<div class="container-fluid main-container">


<div id="header">
<h1 class="title">Machine Learning 010 Project Writeup</h1>
<h4 class="author"><em>Toby Patterson</em></h4>
<h4 class="date"><em>January 23, 2015</em></h4>
</div>


<div id="executive-summary" class="section level2">
<h2>Executive Summary</h2>
<p>In this analysis we will try to predict 20 observations based on data from personal fitness devices.</p>
</div>
<div id="exploratory-analysis" class="section level2">
<h2>Exploratory Analysis</h2>
<p>The data for our analysis has been partitioned into training and test datasets already. Our first step will be to load the data, and then remove any of observations that are not complete. We’ll also remove the first five datapoints which are not device measurements.</p>
<pre class="r"><code>training   = read.csv(&quot;pml-training.csv&quot;, stringsAsFactors=F)
validation = read.csv(&quot;pml-testing.csv&quot;, stringsAsFactors=F)
completeCasesByVariables = complete.cases(t(training))
completeCasesByVariables[1:7] = F
training   = training[,completeCasesByVariables]
validation = validation[,completeCasesByVariables]</code></pre>
<p>The training set now contains 19622 and variables 86 variables, but some of these contain no variation or are covarient with other variables, and thus should be removed. Note that we are also removing the variables from the validation set.</p>
<pre class="r"><code>nzvVariables = nearZeroVar(training, uniqueCut=10)
training     = training[,-nzvVariables]
validation   = validation[,-nzvVariables]

M = abs(cor(training[,-dim(training)[2]]))
diag(M) = 0
corVariables = findCorrelation(M, .7)
training   = training[,-corVariables]
validation = validation[,-corVariables]</code></pre>
<p>The processed training dataset now contains 19622 observations and variables 31 variables.</p>
</div>
<div id="data-analysis" class="section level2">
<h2>Data Analysis</h2>
<p>We’ll treat the provided test set as our <em>validation</em> dataset, and partition the provided training data into our own training and test datasets.</p>
<pre class="r"><code>inTrain = createDataPartition(y=training$classe, p=.75, list=F)
trainingSet = training[inTrain,]
testingSet  = training[-inTrain,]</code></pre>
<p>Next will train the model using all remaining variables using the <em>randomForest</em> algorithm with the default options, and build a prediction based on our testing dataset.</p>
<pre class="r"><code>trainingSet$classe = as.factor(trainingSet$classe)
resultColumn = dim(testingSet)[2]
fittedModel = randomForest(classe ~ ., data = trainingSet)
predictResult = predict(fittedModel, testingSet[,-resultColumn])
modelAccuracy = confusionMatrix(testingSet[,resultColumn], predictResult)
modelAccuracy$overall</code></pre>
<pre><code>##       Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull 
##      0.9871533      0.9837487      0.9835931      0.9901145      0.2846656 
## AccuracyPValue  McnemarPValue 
##      0.0000000            NaN</code></pre>
<p>Our model provides a 0.9871533 accuracy, which is pleasently sufficient, and predicts most variables very well.</p>
<pre class="r"><code>correctPredictions = testingSet$classe == predictResult
table(predictResult, testingSet$classe)</code></pre>
<pre><code>##              
## predictResult    A    B    C    D    E
##             A 1392    4    0    0    0
##             B    2  938   14    0    0
##             C    0    7  836   28    0
##             D    0    0    5  774    0
##             E    1    0    0    2  901</code></pre>
<p>Let’s have a look at our 5 most important predictors.</p>
<pre class="r"><code>variableImportance = varImp(fittedModel)
rownames(variableImportance)[order(variableImportance, decreasing = T)][1:15]</code></pre>
<pre><code>##  [1] &quot;magnet_dumbbell_z&quot;    &quot;pitch_forearm&quot;        &quot;magnet_belt_z&quot;       
##  [4] &quot;roll_forearm&quot;         &quot;roll_dumbbell&quot;        &quot;gyros_belt_z&quot;        
##  [7] &quot;roll_arm&quot;             &quot;yaw_dumbbell&quot;         &quot;gyros_dumbbell_y&quot;    
## [10] &quot;total_accel_dumbbell&quot; &quot;accel_forearm_x&quot;      &quot;magnet_forearm_z&quot;    
## [13] &quot;magnet_arm_x&quot;         &quot;accel_forearm_z&quot;      &quot;pitch_dumbbell&quot;</code></pre>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
</div>
<div id="appendix" class="section level2">
<h2>Appendix</h2>
<p>Plot of must important predictors and their cross-validation importance.</p>
<pre class="r"><code>varImpPlot(fittedModel, n.var = 10)</code></pre>
<p><img src="Machine_Learning_010_-_Project_Writeup_files/figure-html/unnamed-chunk-10-1.png" title="" alt="" width="672" style="display: block; margin: auto;" /></p>
<p>Plot the error rate over various outcomes and interations of the tree.</p>
<pre class="r"><code>plot(fittedModel, log=&quot;y&quot;)
legend(&quot;topright&quot;, colnames(fittedModel$err.rate),col=1:5,fill=1:5)</code></pre>
<p><img src="Machine_Learning_010_-_Project_Writeup_files/figure-html/unnamed-chunk-11-1.png" title="" alt="" width="672" style="display: block; margin: auto;" /></p>
<p>Examine how our top three predictors relate to each other</p>
<pre class="r"><code>variableImportance = varImp(fittedModel)
tenMostImportantVariables = rownames(variableImportance)[order(variableImportance, decreasing = T)][1:10]

testingSet$classe = as.factor(testingSet$classe)
featurePlot(x=testingSet[, tenMostImportantVariables[1:3]], y=testingSet$classe, plot=&quot;pairs&quot;)</code></pre>
<p><img src="Machine_Learning_010_-_Project_Writeup_files/figure-html/unnamed-chunk-12-1.png" title="" alt="" width="672" style="display: block; margin: auto;" /></p>
<p>General information about accuracy, confidence, and error.</p>
<pre class="r"><code>modelAccuracy</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1392    2    0    0    1
##          B    4  938    7    0    0
##          C    0   14  836    5    0
##          D    0    0   28  774    2
##          E    0    0    0    0  901
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9872          
##                  95% CI : (0.9836, 0.9901)
##     No Information Rate : 0.2847          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9837          
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9971   0.9832   0.9598   0.9936   0.9967
## Specificity            0.9991   0.9972   0.9953   0.9927   1.0000
## Pos Pred Value         0.9978   0.9884   0.9778   0.9627   1.0000
## Neg Pred Value         0.9989   0.9960   0.9914   0.9988   0.9993
## Prevalence             0.2847   0.1945   0.1776   0.1588   0.1843
## Detection Rate         0.2838   0.1913   0.1705   0.1578   0.1837
## Detection Prevalence   0.2845   0.1935   0.1743   0.1639   0.1837
## Balanced Accuracy      0.9981   0.9902   0.9776   0.9932   0.9983</code></pre>
</div>
<div id="reference" class="section level2">
<h2>Reference</h2>
<p>Dataset and citations: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a></p>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
