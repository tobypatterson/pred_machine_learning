---
title: "Machine Learning 010 Project Writeup"
author: "Toby Patterson"
date: "January 23, 2015"
output:
  html_document:
    self_contained: no
---

```{r echo=F,message=F,warning=F}
library(caret)
library(randomForest)
library(kernlab)
library(ggplot2)
library(rattle)
```

## Executive Summary

In this analysis we will try to predict 20 observations based on data from personal fitness devices.

## Exploratory Analysis

The data for our analysis has been partitioned into training and test datasets already. Our first step will be to load the data, and then remove any of observations that are not complete.  We'll also remove the first five datapoints which are not device measurements.

```{r}
training   = read.csv("pml-training.csv", stringsAsFactors=F)
validation = read.csv("pml-testing.csv", stringsAsFactors=F)
completeCasesByVariables = complete.cases(t(training))
completeCasesByVariables[1:7] = F
training   = training[,completeCasesByVariables]
validation = validation[,completeCasesByVariables]
```


```{r echo=F}
trainingSize   = dim(training)
validationSize = dim(validation)
```

The training set now contains `r trainingSize[1]` and variables `r trainingSize[2]` variables, but some of these contain no variation or are covarient with other variables, and thus should be removed. Note that we are also removing the variables from the validation set.

```{r}
nzvVariables = nearZeroVar(training, uniqueCut=10)
training     = training[,-nzvVariables]
validation   = validation[,-nzvVariables]

M = abs(cor(training[,-dim(training)[2]]))
diag(M) = 0
corVariables = findCorrelation(M, .7)
training   = training[,-corVariables]
validation = validation[,-corVariables]
```

```{r echo=F}
trainingSize   = dim(training)
validationSize = dim(validation)
```

The processed training dataset now contains `r trainingSize[1]` observations and variables `r trainingSize[2]` variables. 

## Data Analysis

We'll treat the provided test set as our _validation_ dataset, and partition the provided training data into our own training and test datasets.

```{r}
inTrain = createDataPartition(y=training$classe, p=.75, list=F)
trainingSet = training[inTrain,]
testingSet  = training[-inTrain,]
```

Next will train the model using all remaining variables using the _randomForest_ algorithm with the default options, and build a prediction based on our testing dataset.

```{r}
trainingSet$classe = as.factor(trainingSet$classe)
resultColumn = dim(testingSet)[2]
fittedModel = randomForest(classe ~ ., data = trainingSet)
predictResult = predict(fittedModel, testingSet[,-resultColumn])
modelAccuracy = confusionMatrix(testingSet[,resultColumn], predictResult)
modelAccuracy$overall
```

Our model provides a `r modelAccuracy$overall[1]` accuracy, which is pleasently sufficient, and predicts most variables very well.

```{r}

correctPredictions = testingSet$classe == predictResult
table(predictResult, testingSet$classe)
```

Let's have a look at our 5 most important predictors.
```{r}
variableImportance = varImp(fittedModel)
rownames(variableImportance)[order(variableImportance, decreasing = T)][1:15]
```
## Conclusion

## Appendix

Plot of must important predictors and their cross-validation importance.
```{r fig.align='center'}
varImpPlot(fittedModel, n.var = 10)
```

Plot the error rate over various outcomes and interations of the tree.
```{r fig.align='center'}
plot(fittedModel, log="y")
legend("topright", colnames(fittedModel$err.rate),col=1:5,fill=1:5)
```

Examine how our top three predictors relate to each other
```{r fig.align='center'}
variableImportance = varImp(fittedModel)
tenMostImportantVariables = rownames(variableImportance)[order(variableImportance, decreasing = T)][1:10]

testingSet$classe = as.factor(testingSet$classe)
featurePlot(x=testingSet[, tenMostImportantVariables[1:3]], y=testingSet$classe, plot="pairs")
```

General information about accuracy, confidence, and error.

```{r}
modelAccuracy
```

## Reference

Dataset and citations: http://groupware.les.inf.puc-rio.br/har